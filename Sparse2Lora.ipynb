{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhuWJXn7X7-Z",
        "outputId": "ffd31e7b-237d-4f1d-e7e9-5e49031765d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sparsegpt'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 37 (delta 18), reused 8 (delta 8), pack-reused 11\u001b[K\n",
            "Receiving objects: 100% (37/37), 21.78 KiB | 7.26 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Copyright 2023 Ontocord.AI, Apache 2 License\n",
        "# Create Use sparsification on a specific data distribution, and SVD to create Loras from sparsified network.\n",
        "\n",
        "!git clone https://github.com/IST-DASLab/sparsegpt\n",
        "\n",
        "\n",
        "txt = \"\"\"Abraham Lincoln (/ˈlɪŋkən/ LINK-ən; February 12, 1809 – April 15, 1865) was an American lawyer, politician, and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the Union through the American Civil War to defend the nation as a constitutional union and succeeded in abolishing slavery, bolstering the federal government, and modernizing the U.S. economy.\n",
        "\n",
        "Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his successful law practice in Springfield, Illinois. In 1854, he was angered by the Kansas–Nebraska Act, which opened the territories to slavery, and he re-entered politics. He soon became a leader of the new Republican Party. He reached a national audience in the 1858 Senate campaign debates against Stephen A. Douglas. Lincoln ran for president in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his election as a threat to slavery, and Southern states began seceding from the nation. During this time, the newly formed Confederate States of America began seizing federal military bases in the south. Just over one month after Lincoln assumed the presidency, the Confederate States attacked Fort Sumter, a U.S. fort in South Carolina. Following the bombardment, Lincoln mobilized forces to suppress the rebellion and restore the union.\n",
        "\n",
        "\n",
        "Marriage and children\n",
        "\n",
        "Lincoln had pledged in 1846 to serve only one term in the House. Realizing Clay was unlikely to win the presidency, he supported General Zachary Taylor for the Whig nomination in the 1848 presidential election.[85] Taylor won and Lincoln hoped in vain to be appointed Commissioner of the General Land Office.[86] The administration offered to appoint him secretary or governor of the Oregon Territory as consolation.[87] This distant territory was a Democratic stronghold, and acceptance of the post would have disrupted his legal and political career in Illinois, so he declined and resumed his law practice.[88]\n",
        "\n",
        "Lincoln's second child was named\"\"\"\n",
        "\n",
        "try:\n",
        "  import accelerate, bitsandbytes\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "except:\n",
        "  !pip install -q transformers accelerate bitsandbytes\n",
        "  !pip install -q datasets\n",
        "  !pip install -q sentencepiece\n",
        "  !pip install -q zstandard\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibVz6SUDbNRT",
        "outputId": "8c75cf1b-e6c3-40fe-8bb0-ea13f7b28991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sparsegpt/datautils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sparsegpt/datautils.py\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LlamaTokenizer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "def get_tokenizer(model):\n",
        "    if \"llama\" in model.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model, use_fast=False)\n",
        "        # fix for transformer 4.28.0.dev0 compatibility\n",
        "        if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
        "            try:\n",
        "                tokenizer.bos_token_id = 1\n",
        "                tokenizer.eos_token_id = 2\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    return tokenizer\n",
        "\n",
        "def get_wikitext2(nsamples, seed, seqlen, model, tokenizer):\n",
        "\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_ptb(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
        "    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n",
        "    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "import tqdm\n",
        "def get_c4(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train'\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation'\n",
        "    )\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "\n",
        "def get_generic(nsamples, seed, seqlen, model, tokenizer, dataset_name, train, validation):\n",
        "\n",
        "    traindata = load_dataset(\n",
        "        dataset_name, split=train,\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        dataset_name, split=validation,\n",
        "    )\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in tqdm.tqdm(range(nsamples)):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n",
        "    tokenizer = get_tokenizer(model)\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, model, tokenizer)\n",
        "    elif 'ptb' in name:\n",
        "        return get_ptb(nsamples, seed, seqlen, model, tokenizer)\n",
        "    elif 'c4' in name:\n",
        "        return get_c4(nsamples, seed, seqlen, model, tokenizer)\n",
        "    else:\n",
        "        name, train, validiation = name.split(\",\")\n",
        "        return get_generic(nsamples, seed, seqlen, model, tokenizer, name, train, validiation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3oKaq9mos6",
        "outputId": "208a8164-d6d8-4dd4-8c44-6e15e030fd6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sparsegpt/llama.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sparsegpt/llama.py\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sparsegpt import *\n",
        "from modelutils import *\n",
        "from quant import *\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "    has_wandb = True\n",
        "except:\n",
        "    has_wandb = False\n",
        "\n",
        "\n",
        "def get_llama(model):\n",
        "    import torch\n",
        "    def skip(*args, **kwargs):\n",
        "        pass\n",
        "    torch.nn.init.kaiming_uniform_ = skip\n",
        "    torch.nn.init.uniform_ = skip\n",
        "    torch.nn.init.normal_ = skip\n",
        "    from transformers import LlamaForCausalLM\n",
        "    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n",
        "    model.seqlen = 2048\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_sequential(model, dataloader, dev):\n",
        "    print(\"Starting...\")\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
        "    model.model.norm = model.model.norm.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {\"i\": 0, \"attention_mask\": None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache[\"i\"]] = inp\n",
        "            cache[\"i\"] += 1\n",
        "            cache[\"attention_mask\"] = kwargs[\"attention_mask\"]\n",
        "            raise ValueError\n",
        "\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0].to(dev))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
        "    model.model.norm = model.model.norm.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache[\"attention_mask\"]\n",
        "\n",
        "    print(\"Ready.\")\n",
        "\n",
        "    quantizers = {}\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i].to(dev)\n",
        "        full = find_layers(layer)\n",
        "\n",
        "        if args.true_sequential:\n",
        "            sequential = [\n",
        "                [\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\"],\n",
        "                [\"self_attn.o_proj\"],\n",
        "                [\"mlp.up_proj\", \"mlp.gate_proj\"],\n",
        "                [\"mlp.down_proj\"],\n",
        "            ]\n",
        "        else:\n",
        "            sequential = [list(full.keys())]\n",
        "\n",
        "        for names in sequential:\n",
        "            subset = {n: full[n] for n in names}\n",
        "\n",
        "            gpts = {}\n",
        "            for name in subset:\n",
        "                if (\n",
        "                    not (args.minlayer <= i < args.maxlayer and args.prune_only in name)\n",
        "                ) == (not args.invert):\n",
        "                    continue\n",
        "                gpts[name] = SparseGPT(subset[name])\n",
        "                if args.wbits < 16:\n",
        "                    gpts[name].quantizer = Quantizer()\n",
        "                    gpts[name].quantizer.configure(\n",
        "                        args.wbits, perchannel=True, sym=False, mse=False\n",
        "                    )\n",
        "\n",
        "            def add_batch(name):\n",
        "                def tmp(_, inp, out):\n",
        "                    gpts[name].add_batch(inp[0].data, out.data)\n",
        "\n",
        "                return tmp\n",
        "\n",
        "            handles = []\n",
        "            for name in subset:\n",
        "                handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
        "            for j in range(args.nsamples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "\n",
        "            for name in subset:\n",
        "                print(i, name)\n",
        "                print(\"Pruning ...\")\n",
        "                sparsity = args.sparsity\n",
        "                gpts[name].fasterprune(\n",
        "                    sparsity,\n",
        "                    prunen=args.prunen,\n",
        "                    prunem=args.prunem,\n",
        "                    percdamp=args.percdamp,\n",
        "                    blocksize=args.blocksize,\n",
        "                )\n",
        "                gpts[name].free()\n",
        "\n",
        "        for j in range(args.nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        del gpts\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "    return quantizers\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, testenc, dev,  dataset: str, log_wandb: bool = False):\n",
        "    print(\"Evaluating ...\")\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // model.seqlen\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {\"i\": 0, \"attention_mask\": None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache[\"i\"]] = inp\n",
        "            cache[\"i\"] += 1\n",
        "            cache[\"attention_mask\"] = kwargs[\"attention_mask\"]\n",
        "            raise ValueError\n",
        "\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * model.seqlen) : ((i + 1) * model.seqlen)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache[\"attention_mask\"]\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        layer = layers[i].to(dev)\n",
        "\n",
        "        if args.gmp:\n",
        "            subset = find_layers(layer)\n",
        "            for name in subset:\n",
        "                W = subset[name].weight.data\n",
        "                thresh = torch.sort(torch.abs(W.flatten()))[0][\n",
        "                    int(W.numel() * args.sparsity)\n",
        "                ]\n",
        "                W.data[torch.abs(W.data) <= thresh] = 0\n",
        "\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.norm is not None:\n",
        "        model.model.norm = model.model.norm.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.norm is not None:\n",
        "            hidden_states = model.model.norm(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[:, (i * model.seqlen) : ((i + 1) * model.seqlen)][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
        "        )\n",
        "        neg_log_likelihood = loss.float() * model.seqlen\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
        "    print(f\"Perplexity: {ppl.item():3f}\")\n",
        "    if log_wandb:\n",
        "        wandb.log({f\"{dataset}/perplexity\": ppl.item()})\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    from datautils import *\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"model\", type=str, help=\"LlaMA model to load\")\n",
        "    parser.add_argument(\n",
        "        \"dataset\",\n",
        "        type=str,\n",
        "        #choices=[\"wikitext2\", \"ptb\", \"c4\"],\n",
        "        help=\"Where to extract calibration data from.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=0, help=\"Seed for sampling the calibration data.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nsamples\", type=int, default=128, help=\"Number of calibration data samples.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--percdamp\",\n",
        "        type=float,\n",
        "        default=0.01,\n",
        "        help=\"Percent of the average Hessian diagonal to use for dampening.\",\n",
        "    )\n",
        "    parser.add_argument(\"--sparsity\", type=float, default=0, help=\"Target sparsity\")\n",
        "    parser.add_argument(\"--prunen\", type=int, default=0, help=\"N for N:M pruning.\")\n",
        "    parser.add_argument(\"--prunem\", type=int, default=0, help=\"M for N:M pruning.\")\n",
        "    parser.add_argument(\n",
        "        \"--blocksize\",\n",
        "        type=int,\n",
        "        default=128,\n",
        "        help=\"Blocksize to use for adaptive mask selection.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gmp\", action=\"store_true\", help=\"Whether to run the GMP baseline.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--wbits\", type=int, default=16, help=\"Whether to quantize as well.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--minlayer\", type=int, default=-1, help=\"Prune all layers with id >= this.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--maxlayer\", type=int, default=1000, help=\"Prune all layers with id < this.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prune_only\",\n",
        "        type=str,\n",
        "        default=\"\",\n",
        "        help=\"Prune only layers that contain this text.\",\n",
        "    )\n",
        "    parser.add_argument(\"--invert\", action=\"store_true\", help=\"Invert subset.\")\n",
        "    parser.add_argument(\"--save\", type=str, default=\"\", help=\"Path to saved model.\")\n",
        "    parser.add_argument(\n",
        "        \"--true-sequential\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to run in true sequential model.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log_wandb\", action=\"store_true\", help=\"Whether to log to wandb.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # init W&B logging\n",
        "    if args.log_wandb:\n",
        "        assert has_wandb, \"wandb not installed try `pip install wandb`\"\n",
        "        wandb.init(config=args)\n",
        "\n",
        "    model = get_llama(args.model)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader, testloader = get_loaders(\n",
        "        args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
        "    )\n",
        "\n",
        "    if (args.sparsity or args.prunen) and not args.gmp:\n",
        "        tick = time.time()\n",
        "        llama_sequential(model, dataloader, DEV)\n",
        "        for n, p in model.named_parameters():\n",
        "            print(n, torch.mean((p == 0).float()))\n",
        "            if 'down_proj' in n:\n",
        "                break\n",
        "        print(time.time() - tick)\n",
        "\n",
        "    for dataset in [\"wikitext2\", \"ptb\", \"c4\"]:\n",
        "        dataloader, testloader = get_loaders(\n",
        "            dataset, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
        "        )\n",
        "        print(\"Dataset:\", dataset)\n",
        "        llama_eval(model, testloader, DEV, dataset, args.log_wandb)\n",
        "\n",
        "    if args.save:\n",
        "        model.save_pretrained(args.save)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G29jkwCtqmEe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoraLinear(nn.Linear):\n",
        "  def __init__(self, in_features, out_features, bias, linear, lora):\n",
        "    super().__init__(in_features, out_features, bias)\n",
        "    self.weight.data = linear.weight.data\n",
        "    if bias:\n",
        "      self.bias.data = linear.bias.data\n",
        "    self.lora = lora\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    #print(f\"input_tensor.shape = {input_tensor.shape}\")\n",
        "    out = super().forward(input_tensor)\n",
        "    #print(f\"out.shape = {out.shape} , self.lora(input_tensor).shape = {self.lora(input_tensor).shape}\")\n",
        "    out = (out + self.lora(input_tensor))/2.0\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Az9-G-otqyuQ"
      },
      "outputs": [],
      "source": [
        "def create_factorized_compression_for_linear(source_linear, rank=None, rank_factor=0.3,  dtype=torch.float32):\n",
        "    with torch.no_grad():\n",
        "      if rank is None:\n",
        "        rank = max(1, int(min(source_linear.weight.shape)*rank_factor))\n",
        "      if hasattr(source_linear, 'bias'):\n",
        "        bias = source_linear.bias\n",
        "      else:\n",
        "        bias = None\n",
        "      source_linear = source_linear.weight.data\n",
        "      device=source_linear.device\n",
        "      assert rank < min(source_linear.shape)\n",
        "      source_linear = source_linear.float()\n",
        "      U, S, Vh = torch.linalg.svd(source_linear)\n",
        "      U = U[:, :rank]\n",
        "      S = S[:rank]\n",
        "      U = U @ torch.diag(S)\n",
        "      Vh = Vh[:rank, :]\n",
        "      U_flatten = U.flatten()\n",
        "      Vh_flatten = Vh.flatten()\n",
        "      max_quant_size = 2^23\n",
        "      #print (\"ranked\")\n",
        "      if len(U_flatten) + len(Vh_flatten) >= max_quant_size:\n",
        "        dist2 = U_flatten[:min(len(U_flatten), max_quant_size)]\n",
        "        dist3 = Vh_flatten[:min(len(Vh_flatten), max_quant_size)]\n",
        "        hi_val = max(torch.quantile(dist3, 1), torch.quantile(dist2, 1))\n",
        "      else:\n",
        "        dist = torch.cat([U_flatten, Vh_flatten])\n",
        "        hi_val = torch.quantile(dist, 1)\n",
        "      low_val = -hi_val\n",
        "      #print (\"quantile\")\n",
        "      U = U.clamp(low_val, hi_val)\n",
        "      Vh = Vh.clamp(low_val, hi_val)\n",
        "      #print (\"clammped\")\n",
        "      print(f\"U.shape = {U.shape}\")\n",
        "      print(f\"Vh.shape = {Vh.shape}\")\n",
        "\n",
        "      lora_down = nn.Linear(Vh.shape[1], Vh.shape[0], dtype=dtype, bias=False, device=source_linear.device)\n",
        "      lora_up = nn.Linear(U.shape[1], U.shape[0], dtype=dtype, bias=bias is not None, device=source_linear.device)\n",
        "      #print (\"Set up linear\")\n",
        "      lora_up.weight.data = U.to(device=device, dtype=dtype)\n",
        "      lora_down.weight.data = Vh.to(device=device, dtype=dtype)\n",
        "      if bias is not None:\n",
        "        lora_up.bias = nn.Parameter(bias.clone())\n",
        "      return nn.Sequential(lora_down, lora_up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v01GGPjNqw4B"
      },
      "outputs": [],
      "source": [
        "import cupy\n",
        "\n",
        "def lord_decompose(layer, proxy_data, rank):\n",
        "    \"\"\"\n",
        "    Be aware when performing LoRD/AFM on Square Matrices:\n",
        "\n",
        "        In square matrices, input and output dimensions are coupled so compression is more challenging.\n",
        "        The optimal low rank approximation may differ significantly from the original matrix due to the coupling.\n",
        "        Square matrices have less intrinsic redundancy between inputs and outputs to exploit.\n",
        "        Decomposing square matrices risks distorting dimensions that interact in complex ways.\n",
        "        The approximation error of AFM tends to be lowest for tall matrices and highest for square ones.\n",
        "        For square matrices, it can help to decompose blocks of interactions rather than the whole matrix.\n",
        "\n",
        "    Paper reference:\n",
        "        Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression\n",
        "        ( http://arxiv.org/abs/2309.14021 )\n",
        "\n",
        "    Credit: AI chatbot\n",
        "    \"\"\"\n",
        "\n",
        "    y = layer(proxy_data) # Forward proxy data\n",
        "\n",
        "    cov_y = torch.cov(y.T) # Compute output covariance\n",
        "    cov_y = cov_y.float()\n",
        "\n",
        "    # The matrix to be eigendecomposed is symmetric,\n",
        "    # so we can use torch.linalg.eigh instead of torch.linalg.eig\n",
        "    # We shouldn't get imaginary part.\n",
        "    eigenvalues, eigenvectors = torch.linalg.eigh(cov_y)\n",
        "\n",
        "    # Allocate CuPy array\n",
        "    eigenvalues_cupy = cupy.empty(eigenvalues.shape, dtype=cupy.complex64)\n",
        "\n",
        "    # Copy with CUDA stream\n",
        "    eigenvalues_cupy = cupy.from_dlpack(torch.utils.dlpack.to_dlpack(eigenvalues))\n",
        "\n",
        "    # Take top rank eigenvectors in descending order\n",
        "    # [-rank:] selects the last rank indices, which correspond to the largest rank values.\n",
        "    # [::-1] then reverses the array to give the indices that would sort eigenvalues_cupy in descending order.\n",
        "    top_idx = cupy.argsort(cupy.abs(eigenvalues_cupy))[-rank:][::-1]\n",
        "\n",
        "    # PyTorch (eigenvectors) and CuPy (top_idx) arrays can't be indexed against each other like this due to incompatible types.\n",
        "    top_idx_pt = torch.from_numpy(top_idx.get())\n",
        "    U = eigenvectors[:, top_idx_pt]\n",
        "\n",
        "    # Convert layer weight to complex before decomposition, needed if using torch.linalg.eig()\n",
        "    #layer.weight = nn.Parameter(layer.weight.to(torch.complex64))\n",
        "    layer.weight = nn.Parameter(layer.weight.to(torch.float32))\n",
        "\n",
        "    # Decompose\n",
        "    w1 = U.T @ layer.weight\n",
        "    w2 = U\n",
        "    #print(f\"w1.shape = {w1.shape}\")\n",
        "    #print(f\"w2.shape = {w2.shape}\")\n",
        "\n",
        "    # Create LoRD layers\n",
        "    lord_up = nn.Linear(w2.shape[1], w2.shape[0])\n",
        "    lord_down = nn.Linear(w1.shape[1], w1.shape[0])\n",
        "\n",
        "    lord_up.weight.data = w2\n",
        "    lord_down.weight.data = w1\n",
        "    #lord_up.weight.data = torch.real(torch.abs(w2))\n",
        "    #lord_down.weight.data = torch.real(torch.abs(w1))\n",
        "\n",
        "    return nn.Sequential(lord_down, lord_up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "4d2b4fa18c544a6d86ad2b2a2d6d2e7a",
            "97c80a704dd9448f879832d5ec2d3004",
            "efcc3a17c6d04933bd8481bcece7d5f7",
            "7a48769c15d84393b72d1abeaecc2055",
            "5abdca3fc7e248ad8f5210d4a0bd9bca",
            "6312d0a60c934904a0c0e0d8352f973d",
            "75d0fabb1ebf484680173c864ae18779",
            "fefc61d187af461a84c52ce3765d7288",
            "bfc4754d7c2f4772932e7f85629361c3",
            "2ed7e07344d94b4e907131d0ee9f9661",
            "496a3de21aa049cc8e818b3d63c15421",
            "a4bdcc74102b4f3c8b06617884ab4c05",
            "83c80739332c4bf3843f7fd7f70d30a5",
            "55075db0a3324b039ebf25d8e6fac40c",
            "6b8433e95f144050b400363974b269c2",
            "7bef9b96769046cd97669bedd4c3d0a5",
            "e3d8ea0039eb407c963d376946cef1f8",
            "a3dac97ff05d4bfa81b0e1a701bc01b2",
            "e9956d5603284d148b3352d2c3f1fce9",
            "1f601376d7774dc2a904cdf5277b1d77",
            "43dd4abd9e784393b7eb35ab36a9b7ab",
            "adf2373ca9044f9196668a457aba0a82",
            "85e0ae9eb44d466c819f2ea375cd2ff4",
            "def3ce28e13f41ff9f3970aa27c3bbf1",
            "a1ea47e6a1d54cabb9081ea672a92b40",
            "3bd036f780404733aaba57157f9d0e4e",
            "c1aaef3dfb2c4ae0affec4bf6e20b09c",
            "80c786babb684b7db041999b0af11322",
            "8276fb8b2ed14dc58b56784f83c4ef2f",
            "97a1838327cf4749b80d9c564a3a5e03",
            "e0c79ecbf7f34134964b0aedf8b85eda",
            "803cf66025694fa89262f89fa28a2f33",
            "535dfd3dc35240b9bc3c12e971fed925",
            "23b0a77674d44445a54884beada22f96",
            "8fb08736ae504781bdb9d26ac5a05fee",
            "f33dc9826f9441d88c294a5c8774fdbf",
            "b1150952d6aa4ccf9712e2924c3b540f",
            "4f97e7fed60d40f19e44e6406890995b",
            "9bbca0fd33d541dca4455056396a7c3f",
            "6905d9f1c3fc4ee0b1e4ab20c2bccabf",
            "4c800555887f44c2935cabc9473ddfe1",
            "b7f26c7bdf3a438292456e525e17e76d",
            "fce6a6a15b4941eb9301791b958e9c35",
            "2ee514cd955343bfac6a8b0a655f6a26",
            "e04c683c8a54493a8e4da6a79e619edb",
            "5c520af552044a18ab6cfe5215635207",
            "ae7b341c66b4457b9e94fbbda54129b8",
            "68d9e416870d4c97b35583f4a3380b47",
            "f469b93ec1ae49c4b6cb88973ccf8a31",
            "6f107c7ce1c84e409b34e7c47d70f1f4",
            "3cdf7c7aaf204374808ecf5283e18990",
            "0908d4fa906d47dba9c2397e1abb40ae",
            "7ed09a385ab944c0b64c86b8c8251c40",
            "fbdc8c30811a4e0ca9bf2883f18ba12d",
            "b515efbc84bc4ab090c6f4aa122e59ff"
          ]
        },
        "id": "KRL54Os9tDN_",
        "outputId": "eb708e7a-99ab-4e00-9dd1-b606307d1c5b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d2b4fa18c544a6d86ad2b2a2d6d2e7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4bdcc74102b4f3c8b06617884ab4c05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85e0ae9eb44d466c819f2ea375cd2ff4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23b0a77674d44445a54884beada22f96"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e04c683c8a54493a8e4da6a79e619edb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.models.gpt_neox.configuration_gpt_neox import *\n",
        "\n",
        "\n",
        "model_name = \"EleutherAI/pythia-410m\" #\"EleutherAI/pythia-70m\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model.embed_out  = create_factorized_compression_for_linear(model.embed_out, rank_factor=0.9).cuda().to(torch.bfloat16)\n",
        "#model.gpt_neox.embed_in = create_factorized_compression_for_linear(model.gpt_neox.embed_in, rank_factor=0.2).cuda().to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qnwVwUyeq-SV"
      },
      "outputs": [],
      "source": [
        "# Generate proxy dataset\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "proxy_texts = [\n",
        "  \"The cat sat on the mat.\",\n",
        "  \"The quick brown fox jumps over the lazy dog.\",\n",
        "  \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
        "  \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\",\n",
        "  \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\",\n",
        "  \"In a hole in the ground there lived a hobbit.\",\n",
        "  \"Happy families are all alike; every unhappy family is unhappy in its own way.\",\n",
        "  \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of light, it was the season of darkness, it was the spring of hope, it was the winter of despair.\",\n",
        "  \"I was born in the year 1632, in the city of York, of a good family, though not of that country, my father being a foreigner of Bremen, who settled first at Hull.\",\n",
        "  \"Many years later, as he faced the firing squad, Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice.\",\n",
        "  \"The sun shone, having no alternative, on the nothing new.\",\n",
        "  \"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\",\n",
        "  \"When he was nearly thirteen, my brother Jem got his arm badly broken at the elbow.\",\n",
        "  \"There was a boy called Eustace Clarence Scrubb, and he almost deserved it.\",\n",
        "  \"The primroses were over.\",\n",
        "  \"Many years later, as he faced the firing squad, Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice.\",\n",
        "  \"Riverrun, past Eve and Adam's, from swerve of shore to bend of bay, brings us by a commodius vicus of recirculation back to Howth Castle and Environs.\",\n",
        "  \"124 was spiteful.\",\n",
        "  \"The cold passed reluctantly from the earth, and the retiring fogs revealed an army stretched out on the hills, resting.\",\n",
        "  \"We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender.\",\n",
        "  \"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\",\n",
        "  \"Ask not what your country can do for you – ask what you can do for your country.\",\n",
        "  \"The only thing we have to fear is fear itself.\",\n",
        "  \"I do not agree with what you have to say, but I'll defend to the death your right to say it.\",\n",
        "  \"The future belongs to those who believe in the beauty of their dreams.\",\n",
        "  \"It does not matter how slowly you go as long as you do not stop.\"\n",
        "  \"Be who you are and say what you feel, because those who mind don't matter and those who matter don't mind.\",\n",
        "  \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\",\n",
        "  # Add more samples from diverse books, speeches, genres etc.\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QQZuy6HqzQs",
        "outputId": "0bbe548e-7c5e-43dc-c568-5fe8c8e59bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(model.gpt_neox.layers) = 24\n",
            "now processing layer [0]\n",
            "now processing layer [2]\n",
            "now processing layer [4]\n",
            "now processing layer [6]\n",
            "now processing layer [8]\n",
            "now processing layer [10]\n",
            "now processing layer [12]\n",
            "now processing layer [14]\n",
            "now processing layer [16]\n",
            "now processing layer [18]\n",
            "now processing layer [20]\n",
            "now processing layer [22]\n"
          ]
        }
      ],
      "source": [
        "USE_LORD = 1\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "if USE_LORD:\n",
        "  rank_factor=0.9\n",
        "\n",
        "  print(f\"len(model.gpt_neox.layers) = {len(model.gpt_neox.layers)}\")\n",
        "\n",
        "  #for layer in model.gpt_neox.layers:\n",
        "  for i in range(0, len(model.gpt_neox.layers), 2):\n",
        "    layer = model.gpt_neox.layers[i]\n",
        "\n",
        "    print(f\"now processing layer [{i}]\")\n",
        "\n",
        "    input_dim = layer.attention.dense.in_features\n",
        "    rank = int(input_dim * rank_factor)\n",
        "\n",
        "\n",
        "    max_length_dense_h_to_4h = layer.mlp.dense_h_to_4h.in_features # Max sequence length\n",
        "    proxy_data_h = tokenizer(proxy_texts, padding=\"max_length\", truncation=True, max_length=max_length_dense_h_to_4h, return_tensors=\"pt\").to(\"cuda\")\n",
        "    proxy_data_h_bf16 = proxy_data_h['input_ids'].to(torch.bfloat16)\n",
        "\n",
        "    max_length_dense_4h_to_h = layer.mlp.dense_4h_to_h.in_features # Max sequence length\n",
        "    proxy_data_4h = tokenizer(proxy_texts, padding=\"max_length\", truncation=True, max_length=max_length_dense_4h_to_h, return_tensors=\"pt\").to(\"cuda\")\n",
        "    proxy_data_4h_bf16 = proxy_data_4h['input_ids'].to(torch.bfloat16)\n",
        "\n",
        "\n",
        "    layer.attention.dense = LoraLinear(layer.attention.dense.in_features, layer.attention.dense.out_features, layer.attention.dense.bias is not None,  layer.attention.dense, \\\n",
        "                                      lord_decompose(layer.attention.dense, proxy_data_h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    #layer.attention.query_key_value = LoraLinear(layer.attention.query_key_value.in_features, layer.attention.query_key_value.out_features, layer.attention.query_key_value.bias is not None, layer.attention.query_key_value, \\\n",
        "    #                                   lord_decompose(layer.attention.query_key_value, proxy_data, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    layer.mlp.dense_h_to_4h = LoraLinear(layer.mlp.dense_h_to_4h.in_features, layer.mlp.dense_h_to_4h.out_features, layer.mlp.dense_h_to_4h.bias is not None, layer.mlp.dense_h_to_4h, \\\n",
        "                                      lord_decompose(layer.mlp.dense_h_to_4h, proxy_data_h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    layer.mlp.dense_4h_to_h = LoraLinear(layer.mlp.dense_4h_to_h.in_features, layer.mlp.dense_4h_to_h.out_features, layer.mlp.dense_4h_to_h.bias is not None, layer.mlp.dense_4h_to_h, \\\n",
        "                                      lord_decompose(layer.mlp.dense_4h_to_h, proxy_data_4h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "else:\n",
        "\n",
        "  #for layer in model.gpt_neox.layers:\n",
        "  for i in range(0, len(model.gpt_neox.layers), 2):\n",
        "    layer = model.gpt_neox.layers[i]\n",
        "\n",
        "    print(f\"now processing layer [{i}]\")\n",
        "\n",
        "    layer.attention.dense = LoraLinear(layer.attention.dense.in_features, layer.attention.dense.out_features, layer.attention.dense.bias is not None,  layer.attention.dense, \\\n",
        "                                      create_factorized_compression_for_linear(layer.attention.dense, rank_factor=0.2)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    #layer.attention.query_key_value = LoraLinear(layer.attention.query_key_value.in_features, layer.attention.query_key_value.out_features, layer.attention.query_key_value.bias is not None, layer.attention.query_key_value, \\\n",
        "    #                                   create_factorized_compression_for_linear(layer.attention.query_key_value, rank_factor=0.5)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    layer.mlp.dense_h_to_4h = LoraLinear(layer.mlp.dense_h_to_4h.in_features, layer.mlp.dense_h_to_4h.out_features, layer.mlp.dense_h_to_4h.bias is not None, layer.mlp.dense_h_to_4h, \\\n",
        "                                      create_factorized_compression_for_linear(layer.mlp.dense_h_to_4h, rank_factor=0.5)).cuda().to(torch.bfloat16)\n",
        "\n",
        "    layer.mlp.dense_4h_to_h = LoraLinear(layer.mlp.dense_4h_to_h.in_features, layer.mlp.dense_4h_to_h.out_features, layer.mlp.dense_4h_to_h.bias is not None, layer.mlp.dense_4h_to_h, \\\n",
        "                                      create_factorized_compression_for_linear(layer.mlp.dense_4h_to_h, rank_factor=0.5)).cuda().to(torch.bfloat16)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(txt, return_tensors=\"pt\").to(\"cuda\")\n",
        "print(f\"input_ids.input_ids.shape = {input_ids.input_ids.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  print(tokenizer.batch_decode(model.generate(**input_ids,  no_repeat_ngram_size=2, repetition_penalty=1.1, min_length=input_ids.input_ids.shape[1]+256, max_new_tokens=512))[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7BxVMUbLp3P",
        "outputId": "6290d54e-9c78-458e-a7c9-dc0e2bf08311"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids.input_ids.shape = torch.Size([1, 477])\n",
            "Abraham Lincoln (/ˈlɪŋkən/ LINK-ən; February 12, 1809 – April 15, 1865) was an American lawyer, politician, and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the Union through the American Civil War to defend the nation as a constitutional union and succeeded in abolishing slavery, bolstering the federal government, and modernizing the U.S. economy.\n",
            "\n",
            "Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his successful law practice in Springfield, Illinois. In 1854, he was angered by the Kansas–Nebraska Act, which opened the territories to slavery, and he re-entered politics. He soon became a leader of the new Republican Party. He reached a national audience in the 1858 Senate campaign debates against Stephen A. Douglas. Lincoln ran for president in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his election as a threat to slavery, and Southern states began seceding from the nation. During this time, the newly formed Confederate States of America began seizing federal military bases in the south. Just over one month after Lincoln assumed the presidency, the Confederate States attacked Fort Sumter, a U.S. fort in South Carolina. Following the bombardment, Lincoln mobilized forces to suppress the rebellion and restore the union.\n",
            "\n",
            "\n",
            "Marriage and children\n",
            "\n",
            "Lincoln had pledged in 1846 to serve only one term in the House. Realizing Clay was unlikely to win the presidency, he supported General Zachary Taylor for the Whig nomination in the 1848 presidential election.[85] Taylor won and Lincoln hoped in vain to be appointed Commissioner of the General Land Office.[86] The administration offered to appoint him secretary or governor of the Oregon Territory as consolation.[87] This distant territory was a Democratic stronghold, and acceptance of the post would have disrupted his legal and political career in Illinois, so he declined and resumed his law practice.[88]\n",
            "\n",
            "Lincoln's second child was named: the presidents and not were leaders of: and the former presidents (and the previous presidents), the w.s.w general, w t.2 2 2 that w w -. the the first w...the thew the will have taken no w…f,, the, has no p.t. and w​f.,\n",
            " the W.& ·, I did not, no P.x,, but w--w -​w, that is, can, or than that, it that was not. n. but not had already been,., and, those we donn. (1874), and that has been not but no other that that may, (, 679 & 919,(1578,74,7576,5,0545,0444,10, 05, 00,   and..3.919, a.4,a,; and a a ( the 2, \" a the y,2 that iva,and,that, 3 + ( a\" a 4,z z z az  | a,03 (   04, 10, \\ 03(  \\ or all.  do, 2.0 or zz ; a 3. ?, |,c 1.;. 2 c.i, tr t ;; t;c  - ) and then,t  • and her,ters. or, t1, s.or,tt,th\\ and;, m.m, in, also, -ma.ma,m.me,ma and -,me.,\", p,91,9490,9590 = and like, with the 3.(.  ).,) and +, means and now, +;\n",
            " a3 ( and 2) (. a4.z  = a;) of, each, (\" a( a)ast a\", and (; a +. - and are, of.es. |.exe; f.f)  .1<, f;  ±, plus,f; c  ; +f), f) or.fa.c a  −. +) f,ff,\" and she passed. ||, i. f• f f • f), •,ft• ft**;ft**) •)•)  )   ; .)\n",
            " ()and. •.•.v, • + v\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_orig = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).cuda()\n",
        "print('compression', sum(x.shape[0]*x.shape[1] if len(x.shape) == 2 else x.shape[0] for x in model.parameters())/ sum(x.shape[0]*x.shape[1] if len(x.shape) == 2 else x.shape[0] for x in model_orig.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDVEgPnFRPL9",
        "outputId": "1611df63-9f9a-4499-8e03-77dffc7b17e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compression 1.335313234604026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "6ggkiW6pLz--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bd004e-059f-4477-e8fb-fbbae3bc0e94"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 249233796 || all params: 541247876 || trainable%: 46.04799520728281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TFUaGmHxkA5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "036596c7-ccf2-40ab-da0f-96e23aab4224"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 1024)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (1): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (2): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (3): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (4): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (5): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (6): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (7): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (8): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (9): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (10): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (11): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (12): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (13): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (14): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (15): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (16): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (17): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (18): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (19): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (20): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (21): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (22): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): LoraLinear(\n",
              "            in_features=1024, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): LoraLinear(\n",
              "            in_features=1024, out_features=4096, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=1024, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=4096, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (dense_4h_to_h): LoraLinear(\n",
              "            in_features=4096, out_features=1024, bias=True\n",
              "            (lora): Sequential(\n",
              "              (0): Linear(in_features=4096, out_features=921, bias=True)\n",
              "              (1): Linear(in_features=921, out_features=1024, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "      (23): GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_orig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9tpn-OU-Dur",
        "outputId": "e4cc145d-a015-4286-db9e-19daf058be89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50304, 1024)\n",
              "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
              "          (query_key_value): Linear(in_features=1024, out_features=3072, bias=True)\n",
              "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (dense_4h_to_h): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=1024, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4d2b4fa18c544a6d86ad2b2a2d6d2e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97c80a704dd9448f879832d5ec2d3004",
              "IPY_MODEL_efcc3a17c6d04933bd8481bcece7d5f7",
              "IPY_MODEL_7a48769c15d84393b72d1abeaecc2055"
            ],
            "layout": "IPY_MODEL_5abdca3fc7e248ad8f5210d4a0bd9bca"
          }
        },
        "97c80a704dd9448f879832d5ec2d3004": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6312d0a60c934904a0c0e0d8352f973d",
            "placeholder": "​",
            "style": "IPY_MODEL_75d0fabb1ebf484680173c864ae18779",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "efcc3a17c6d04933bd8481bcece7d5f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fefc61d187af461a84c52ce3765d7288",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfc4754d7c2f4772932e7f85629361c3",
            "value": 570
          }
        },
        "7a48769c15d84393b72d1abeaecc2055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed7e07344d94b4e907131d0ee9f9661",
            "placeholder": "​",
            "style": "IPY_MODEL_496a3de21aa049cc8e818b3d63c15421",
            "value": " 570/570 [00:00&lt;00:00, 37.5kB/s]"
          }
        },
        "5abdca3fc7e248ad8f5210d4a0bd9bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6312d0a60c934904a0c0e0d8352f973d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d0fabb1ebf484680173c864ae18779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fefc61d187af461a84c52ce3765d7288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfc4754d7c2f4772932e7f85629361c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ed7e07344d94b4e907131d0ee9f9661": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "496a3de21aa049cc8e818b3d63c15421": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4bdcc74102b4f3c8b06617884ab4c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83c80739332c4bf3843f7fd7f70d30a5",
              "IPY_MODEL_55075db0a3324b039ebf25d8e6fac40c",
              "IPY_MODEL_6b8433e95f144050b400363974b269c2"
            ],
            "layout": "IPY_MODEL_7bef9b96769046cd97669bedd4c3d0a5"
          }
        },
        "83c80739332c4bf3843f7fd7f70d30a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3d8ea0039eb407c963d376946cef1f8",
            "placeholder": "​",
            "style": "IPY_MODEL_a3dac97ff05d4bfa81b0e1a701bc01b2",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "55075db0a3324b039ebf25d8e6fac40c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9956d5603284d148b3352d2c3f1fce9",
            "max": 911373632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1f601376d7774dc2a904cdf5277b1d77",
            "value": 911373632
          }
        },
        "6b8433e95f144050b400363974b269c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43dd4abd9e784393b7eb35ab36a9b7ab",
            "placeholder": "​",
            "style": "IPY_MODEL_adf2373ca9044f9196668a457aba0a82",
            "value": " 911M/911M [00:04&lt;00:00, 230MB/s]"
          }
        },
        "7bef9b96769046cd97669bedd4c3d0a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3d8ea0039eb407c963d376946cef1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3dac97ff05d4bfa81b0e1a701bc01b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e9956d5603284d148b3352d2c3f1fce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f601376d7774dc2a904cdf5277b1d77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43dd4abd9e784393b7eb35ab36a9b7ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adf2373ca9044f9196668a457aba0a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85e0ae9eb44d466c819f2ea375cd2ff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_def3ce28e13f41ff9f3970aa27c3bbf1",
              "IPY_MODEL_a1ea47e6a1d54cabb9081ea672a92b40",
              "IPY_MODEL_3bd036f780404733aaba57157f9d0e4e"
            ],
            "layout": "IPY_MODEL_c1aaef3dfb2c4ae0affec4bf6e20b09c"
          }
        },
        "def3ce28e13f41ff9f3970aa27c3bbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80c786babb684b7db041999b0af11322",
            "placeholder": "​",
            "style": "IPY_MODEL_8276fb8b2ed14dc58b56784f83c4ef2f",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "a1ea47e6a1d54cabb9081ea672a92b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a1838327cf4749b80d9c564a3a5e03",
            "max": 396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e0c79ecbf7f34134964b0aedf8b85eda",
            "value": 396
          }
        },
        "3bd036f780404733aaba57157f9d0e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_803cf66025694fa89262f89fa28a2f33",
            "placeholder": "​",
            "style": "IPY_MODEL_535dfd3dc35240b9bc3c12e971fed925",
            "value": " 396/396 [00:00&lt;00:00, 25.0kB/s]"
          }
        },
        "c1aaef3dfb2c4ae0affec4bf6e20b09c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80c786babb684b7db041999b0af11322": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8276fb8b2ed14dc58b56784f83c4ef2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97a1838327cf4749b80d9c564a3a5e03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0c79ecbf7f34134964b0aedf8b85eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "803cf66025694fa89262f89fa28a2f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "535dfd3dc35240b9bc3c12e971fed925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23b0a77674d44445a54884beada22f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fb08736ae504781bdb9d26ac5a05fee",
              "IPY_MODEL_f33dc9826f9441d88c294a5c8774fdbf",
              "IPY_MODEL_b1150952d6aa4ccf9712e2924c3b540f"
            ],
            "layout": "IPY_MODEL_4f97e7fed60d40f19e44e6406890995b"
          }
        },
        "8fb08736ae504781bdb9d26ac5a05fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bbca0fd33d541dca4455056396a7c3f",
            "placeholder": "​",
            "style": "IPY_MODEL_6905d9f1c3fc4ee0b1e4ab20c2bccabf",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "f33dc9826f9441d88c294a5c8774fdbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c800555887f44c2935cabc9473ddfe1",
            "max": 2113710,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7f26c7bdf3a438292456e525e17e76d",
            "value": 2113710
          }
        },
        "b1150952d6aa4ccf9712e2924c3b540f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fce6a6a15b4941eb9301791b958e9c35",
            "placeholder": "​",
            "style": "IPY_MODEL_2ee514cd955343bfac6a8b0a655f6a26",
            "value": " 2.11M/2.11M [00:01&lt;00:00, 1.87MB/s]"
          }
        },
        "4f97e7fed60d40f19e44e6406890995b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9bbca0fd33d541dca4455056396a7c3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6905d9f1c3fc4ee0b1e4ab20c2bccabf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c800555887f44c2935cabc9473ddfe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7f26c7bdf3a438292456e525e17e76d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fce6a6a15b4941eb9301791b958e9c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee514cd955343bfac6a8b0a655f6a26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e04c683c8a54493a8e4da6a79e619edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c520af552044a18ab6cfe5215635207",
              "IPY_MODEL_ae7b341c66b4457b9e94fbbda54129b8",
              "IPY_MODEL_68d9e416870d4c97b35583f4a3380b47"
            ],
            "layout": "IPY_MODEL_f469b93ec1ae49c4b6cb88973ccf8a31"
          }
        },
        "5c520af552044a18ab6cfe5215635207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f107c7ce1c84e409b34e7c47d70f1f4",
            "placeholder": "​",
            "style": "IPY_MODEL_3cdf7c7aaf204374808ecf5283e18990",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "ae7b341c66b4457b9e94fbbda54129b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0908d4fa906d47dba9c2397e1abb40ae",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ed09a385ab944c0b64c86b8c8251c40",
            "value": 99
          }
        },
        "68d9e416870d4c97b35583f4a3380b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbdc8c30811a4e0ca9bf2883f18ba12d",
            "placeholder": "​",
            "style": "IPY_MODEL_b515efbc84bc4ab090c6f4aa122e59ff",
            "value": " 99.0/99.0 [00:00&lt;00:00, 1.51kB/s]"
          }
        },
        "f469b93ec1ae49c4b6cb88973ccf8a31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f107c7ce1c84e409b34e7c47d70f1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdf7c7aaf204374808ecf5283e18990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0908d4fa906d47dba9c2397e1abb40ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed09a385ab944c0b64c86b8c8251c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbdc8c30811a4e0ca9bf2883f18ba12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b515efbc84bc4ab090c6f4aa122e59ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
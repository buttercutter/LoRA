{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhuWJXn7X7-Z",
        "outputId": "c45a5653-01ad-482a-e8c1-96460a7e1dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'sparsegpt'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 37 (delta 18), reused 8 (delta 8), pack-reused 11\u001b[K\n",
            "Receiving objects: 100% (37/37), 21.78 KiB | 5.45 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Copyright 2023 Ontocord.AI, Apache 2 License\n",
        "# Create Use sparsification on a specific data distribution, and SVD to create Loras from sparsified network.\n",
        "\n",
        "!git clone https://github.com/IST-DASLab/sparsegpt\n",
        "\n",
        "\n",
        "txt = \"\"\"Abraham Lincoln (/ˈlɪŋkən/ LINK-ən; February 12, 1809 – April 15, 1865) was an American lawyer, politician, and statesman who served as the 16th president of the United States from 1861 until his assassination in 1865. Lincoln led the Union through the American Civil War to defend the nation as a constitutional union and succeeded in abolishing slavery, bolstering the federal government, and modernizing the U.S. economy.\n",
        "\n",
        "Lincoln was born into poverty in a log cabin in Kentucky and was raised on the frontier, primarily in Indiana. He was self-educated and became a lawyer, Whig Party leader, Illinois state legislator, and U.S. Congressman from Illinois. In 1849, he returned to his successful law practice in Springfield, Illinois. In 1854, he was angered by the Kansas–Nebraska Act, which opened the territories to slavery, and he re-entered politics. He soon became a leader of the new Republican Party. He reached a national audience in the 1858 Senate campaign debates against Stephen A. Douglas. Lincoln ran for president in 1860, sweeping the North to gain victory. Pro-slavery elements in the South viewed his election as a threat to slavery, and Southern states began seceding from the nation. During this time, the newly formed Confederate States of America began seizing federal military bases in the south. Just over one month after Lincoln assumed the presidency, the Confederate States attacked Fort Sumter, a U.S. fort in South Carolina. Following the bombardment, Lincoln mobilized forces to suppress the rebellion and restore the union.\n",
        "\n",
        "\n",
        "Marriage and children\n",
        "\n",
        "Lincoln had pledged in 1846 to serve only one term in the House. Realizing Clay was unlikely to win the presidency, he supported General Zachary Taylor for the Whig nomination in the 1848 presidential election.[85] Taylor won and Lincoln hoped in vain to be appointed Commissioner of the General Land Office.[86] The administration offered to appoint him secretary or governor of the Oregon Territory as consolation.[87] This distant territory was a Democratic stronghold, and acceptance of the post would have disrupted his legal and political career in Illinois, so he declined and resumed his law practice.[88]\n",
        "\n",
        "Lincoln's second child was named\"\"\"\n",
        "\n",
        "try:\n",
        "  import accelerate, bitsandbytes\n",
        "  from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "except:\n",
        "  !pip install -q transformers accelerate bitsandbytes\n",
        "  !pip install -q datasets\n",
        "  !pip install -q sentencepiece\n",
        "  !pip install -q zstandard\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibVz6SUDbNRT",
        "outputId": "9afc214e-6d47-44b3-bd26-598ee8ce4b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sparsegpt/datautils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sparsegpt/datautils.py\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, LlamaTokenizer\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.random.manual_seed(seed)\n",
        "\n",
        "def get_tokenizer(model):\n",
        "    if \"llama\" in model.lower():\n",
        "        tokenizer = LlamaTokenizer.from_pretrained(model, use_fast=False)\n",
        "        # fix for transformer 4.28.0.dev0 compatibility\n",
        "        if tokenizer.bos_token_id != 1 or tokenizer.eos_token_id != 2:\n",
        "            try:\n",
        "                tokenizer.bos_token_id = 1\n",
        "                tokenizer.eos_token_id = 2\n",
        "            except AttributeError:\n",
        "                pass\n",
        "    else:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n",
        "    return tokenizer\n",
        "\n",
        "def get_wikitext2(nsamples, seed, seqlen, model, tokenizer):\n",
        "\n",
        "    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['text']), return_tensors='pt')\n",
        "    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "def get_ptb(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n",
        "    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
        "\n",
        "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n",
        "    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "    return trainloader, testenc\n",
        "\n",
        "import tqdm\n",
        "def get_c4(nsamples, seed, seqlen, model, tokenizer):\n",
        "    traindata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train'\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        'allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation'\n",
        "    )\n",
        "\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in range(nsamples):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "\n",
        "def get_generic(nsamples, seed, seqlen, model, tokenizer, dataset_name, train, validation):\n",
        "\n",
        "    traindata = load_dataset(\n",
        "        dataset_name, split=train,\n",
        "    )\n",
        "    valdata = load_dataset(\n",
        "        dataset_name, split=validation,\n",
        "    )\n",
        "    random.seed(seed)\n",
        "    trainloader = []\n",
        "    for _ in tqdm.tqdm(range(nsamples)):\n",
        "        while True:\n",
        "            i = random.randint(0, len(traindata) - 1)\n",
        "            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n",
        "            if trainenc.input_ids.shape[1] > seqlen:\n",
        "                break\n",
        "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
        "        j = i + seqlen\n",
        "        inp = trainenc.input_ids[:, i:j]\n",
        "        tar = inp.clone()\n",
        "        tar[:, :-1] = -100\n",
        "        trainloader.append((inp, tar))\n",
        "\n",
        "    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n",
        "    valenc = valenc.input_ids[:, :(256 * seqlen)]\n",
        "\n",
        "    class TokenizerWrapper:\n",
        "        def __init__(self, input_ids):\n",
        "            self.input_ids = input_ids\n",
        "    valenc = TokenizerWrapper(valenc)\n",
        "\n",
        "    return trainloader, valenc\n",
        "\n",
        "def get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n",
        "    tokenizer = get_tokenizer(model)\n",
        "    if 'wikitext2' in name:\n",
        "        return get_wikitext2(nsamples, seed, seqlen, model, tokenizer)\n",
        "    elif 'ptb' in name:\n",
        "        return get_ptb(nsamples, seed, seqlen, model, tokenizer)\n",
        "    elif 'c4' in name:\n",
        "        return get_c4(nsamples, seed, seqlen, model, tokenizer)\n",
        "    else:\n",
        "        name, train, validiation = name.split(\",\")\n",
        "        return get_generic(nsamples, seed, seqlen, model, tokenizer, name, train, validiation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d3oKaq9mos6",
        "outputId": "ad1f21bf-cd28-4434-d65c-e09288487e4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting sparsegpt/llama.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile sparsegpt/llama.py\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sparsegpt import *\n",
        "from modelutils import *\n",
        "from quant import *\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "    has_wandb = True\n",
        "except:\n",
        "    has_wandb = False\n",
        "\n",
        "\n",
        "def get_llama(model):\n",
        "    import torch\n",
        "    def skip(*args, **kwargs):\n",
        "        pass\n",
        "    torch.nn.init.kaiming_uniform_ = skip\n",
        "    torch.nn.init.uniform_ = skip\n",
        "    torch.nn.init.normal_ = skip\n",
        "    from transformers import LlamaForCausalLM\n",
        "    model = LlamaForCausalLM.from_pretrained(model, torch_dtype='auto')\n",
        "    model.seqlen = 2048\n",
        "    return model\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_sequential(model, dataloader, dev):\n",
        "    print(\"Starting...\")\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
        "    model.model.norm = model.model.norm.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {\"i\": 0, \"attention_mask\": None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache[\"i\"]] = inp\n",
        "            cache[\"i\"] += 1\n",
        "            cache[\"attention_mask\"] = kwargs[\"attention_mask\"]\n",
        "            raise ValueError\n",
        "\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for batch in dataloader:\n",
        "        try:\n",
        "            model(batch[0].to(dev))\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
        "    model.model.norm = model.model.norm.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache[\"attention_mask\"]\n",
        "\n",
        "    print(\"Ready.\")\n",
        "\n",
        "    quantizers = {}\n",
        "    for i in range(len(layers)):\n",
        "        layer = layers[i].to(dev)\n",
        "        full = find_layers(layer)\n",
        "\n",
        "        if args.true_sequential:\n",
        "            sequential = [\n",
        "                [\"self_attn.k_proj\", \"self_attn.v_proj\", \"self_attn.q_proj\"],\n",
        "                [\"self_attn.o_proj\"],\n",
        "                [\"mlp.up_proj\", \"mlp.gate_proj\"],\n",
        "                [\"mlp.down_proj\"],\n",
        "            ]\n",
        "        else:\n",
        "            sequential = [list(full.keys())]\n",
        "\n",
        "        for names in sequential:\n",
        "            subset = {n: full[n] for n in names}\n",
        "\n",
        "            gpts = {}\n",
        "            for name in subset:\n",
        "                if (\n",
        "                    not (args.minlayer <= i < args.maxlayer and args.prune_only in name)\n",
        "                ) == (not args.invert):\n",
        "                    continue\n",
        "                gpts[name] = SparseGPT(subset[name])\n",
        "                if args.wbits < 16:\n",
        "                    gpts[name].quantizer = Quantizer()\n",
        "                    gpts[name].quantizer.configure(\n",
        "                        args.wbits, perchannel=True, sym=False, mse=False\n",
        "                    )\n",
        "\n",
        "            def add_batch(name):\n",
        "                def tmp(_, inp, out):\n",
        "                    gpts[name].add_batch(inp[0].data, out.data)\n",
        "\n",
        "                return tmp\n",
        "\n",
        "            handles = []\n",
        "            for name in subset:\n",
        "                handles.append(subset[name].register_forward_hook(add_batch(name)))\n",
        "            for j in range(args.nsamples):\n",
        "                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "            for h in handles:\n",
        "                h.remove()\n",
        "\n",
        "            for name in subset:\n",
        "                print(i, name)\n",
        "                print(\"Pruning ...\")\n",
        "                sparsity = args.sparsity\n",
        "                gpts[name].fasterprune(\n",
        "                    sparsity,\n",
        "                    prunen=args.prunen,\n",
        "                    prunem=args.prunem,\n",
        "                    percdamp=args.percdamp,\n",
        "                    blocksize=args.blocksize,\n",
        "                )\n",
        "                gpts[name].free()\n",
        "\n",
        "        for j in range(args.nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        del gpts\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "    return quantizers\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def llama_eval(model, testenc, dev,  dataset: str, log_wandb: bool = False):\n",
        "    print(\"Evaluating ...\")\n",
        "\n",
        "    testenc = testenc.input_ids\n",
        "    nsamples = testenc.numel() // model.seqlen\n",
        "\n",
        "    use_cache = model.config.use_cache\n",
        "    model.config.use_cache = False\n",
        "    layers = model.model.layers\n",
        "\n",
        "    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n",
        "    layers[0] = layers[0].to(dev)\n",
        "\n",
        "    dtype = next(iter(model.parameters())).dtype\n",
        "    inps = torch.zeros(\n",
        "        (nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev\n",
        "    )\n",
        "    cache = {\"i\": 0, \"attention_mask\": None}\n",
        "\n",
        "    class Catcher(nn.Module):\n",
        "        def __init__(self, module):\n",
        "            super().__init__()\n",
        "            self.module = module\n",
        "\n",
        "        def forward(self, inp, **kwargs):\n",
        "            inps[cache[\"i\"]] = inp\n",
        "            cache[\"i\"] += 1\n",
        "            cache[\"attention_mask\"] = kwargs[\"attention_mask\"]\n",
        "            raise ValueError\n",
        "\n",
        "    layers[0] = Catcher(layers[0])\n",
        "    for i in range(nsamples):\n",
        "        batch = testenc[:, (i * model.seqlen) : ((i + 1) * model.seqlen)].to(dev)\n",
        "        try:\n",
        "            model(batch)\n",
        "        except ValueError:\n",
        "            pass\n",
        "    layers[0] = layers[0].module\n",
        "\n",
        "    layers[0] = layers[0].cpu()\n",
        "    model.model.embed_tokens = model.model.embed_tokens.cpu()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    outs = torch.zeros_like(inps)\n",
        "    attention_mask = cache[\"attention_mask\"]\n",
        "\n",
        "    for i in range(len(layers)):\n",
        "        print(i)\n",
        "        layer = layers[i].to(dev)\n",
        "\n",
        "        if args.gmp:\n",
        "            subset = find_layers(layer)\n",
        "            for name in subset:\n",
        "                W = subset[name].weight.data\n",
        "                thresh = torch.sort(torch.abs(W.flatten()))[0][\n",
        "                    int(W.numel() * args.sparsity)\n",
        "                ]\n",
        "                W.data[torch.abs(W.data) <= thresh] = 0\n",
        "\n",
        "        for j in range(nsamples):\n",
        "            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask)[0]\n",
        "        layers[i] = layer.cpu()\n",
        "        del layer\n",
        "        torch.cuda.empty_cache()\n",
        "        inps, outs = outs, inps\n",
        "\n",
        "    if model.model.norm is not None:\n",
        "        model.model.norm = model.model.norm.to(dev)\n",
        "    model.lm_head = model.lm_head.to(dev)\n",
        "\n",
        "    testenc = testenc.to(dev)\n",
        "    nlls = []\n",
        "    for i in range(nsamples):\n",
        "        hidden_states = inps[i].unsqueeze(0)\n",
        "        if model.model.norm is not None:\n",
        "            hidden_states = model.model.norm(hidden_states)\n",
        "        lm_logits = model.lm_head(hidden_states)\n",
        "        shift_logits = lm_logits[:, :-1, :].contiguous()\n",
        "        shift_labels = testenc[:, (i * model.seqlen) : ((i + 1) * model.seqlen)][:, 1:]\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        loss = loss_fct(\n",
        "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
        "        )\n",
        "        neg_log_likelihood = loss.float() * model.seqlen\n",
        "        nlls.append(neg_log_likelihood)\n",
        "    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n",
        "    print(f\"Perplexity: {ppl.item():3f}\")\n",
        "    if log_wandb:\n",
        "        wandb.log({f\"{dataset}/perplexity\": ppl.item()})\n",
        "\n",
        "    model.config.use_cache = use_cache\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    from datautils import *\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"model\", type=str, help=\"LlaMA model to load\")\n",
        "    parser.add_argument(\n",
        "        \"dataset\",\n",
        "        type=str,\n",
        "        #choices=[\"wikitext2\", \"ptb\", \"c4\"],\n",
        "        help=\"Where to extract calibration data from.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=0, help=\"Seed for sampling the calibration data.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--nsamples\", type=int, default=128, help=\"Number of calibration data samples.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--percdamp\",\n",
        "        type=float,\n",
        "        default=0.01,\n",
        "        help=\"Percent of the average Hessian diagonal to use for dampening.\",\n",
        "    )\n",
        "    parser.add_argument(\"--sparsity\", type=float, default=0, help=\"Target sparsity\")\n",
        "    parser.add_argument(\"--prunen\", type=int, default=0, help=\"N for N:M pruning.\")\n",
        "    parser.add_argument(\"--prunem\", type=int, default=0, help=\"M for N:M pruning.\")\n",
        "    parser.add_argument(\n",
        "        \"--blocksize\",\n",
        "        type=int,\n",
        "        default=128,\n",
        "        help=\"Blocksize to use for adaptive mask selection.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gmp\", action=\"store_true\", help=\"Whether to run the GMP baseline.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--wbits\", type=int, default=16, help=\"Whether to quantize as well.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--minlayer\", type=int, default=-1, help=\"Prune all layers with id >= this.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--maxlayer\", type=int, default=1000, help=\"Prune all layers with id < this.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prune_only\",\n",
        "        type=str,\n",
        "        default=\"\",\n",
        "        help=\"Prune only layers that contain this text.\",\n",
        "    )\n",
        "    parser.add_argument(\"--invert\", action=\"store_true\", help=\"Invert subset.\")\n",
        "    parser.add_argument(\"--save\", type=str, default=\"\", help=\"Path to saved model.\")\n",
        "    parser.add_argument(\n",
        "        \"--true-sequential\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to run in true sequential model.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--log_wandb\", action=\"store_true\", help=\"Whether to log to wandb.\"\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # init W&B logging\n",
        "    if args.log_wandb:\n",
        "        assert has_wandb, \"wandb not installed try `pip install wandb`\"\n",
        "        wandb.init(config=args)\n",
        "\n",
        "    model = get_llama(args.model)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader, testloader = get_loaders(\n",
        "        args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
        "    )\n",
        "\n",
        "    if (args.sparsity or args.prunen) and not args.gmp:\n",
        "        tick = time.time()\n",
        "        llama_sequential(model, dataloader, DEV)\n",
        "        for n, p in model.named_parameters():\n",
        "            print(n, torch.mean((p == 0).float()))\n",
        "            if 'down_proj' in n:\n",
        "                break\n",
        "        print(time.time() - tick)\n",
        "\n",
        "    for dataset in [\"wikitext2\", \"ptb\", \"c4\"]:\n",
        "        dataloader, testloader = get_loaders(\n",
        "            dataset, seed=args.seed, model=args.model, seqlen=model.seqlen\n",
        "        )\n",
        "        print(\"Dataset:\", dataset)\n",
        "        llama_eval(model, testloader, DEV, dataset, args.log_wandb)\n",
        "\n",
        "    if args.save:\n",
        "        model.save_pretrained(args.save)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G29jkwCtqmEe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoraLinear(nn.Linear):\n",
        "  def __init__(self, in_features, out_features, bias, linear, lora):\n",
        "    super().__init__(in_features, out_features, bias)\n",
        "    self.weight.data = linear.weight.data\n",
        "    if bias:\n",
        "      self.bias.data = linear.bias.data\n",
        "    self.lora = lora\n",
        "\n",
        "  def forward(self, input_tensor):\n",
        "    out = super().forward(input_tensor)\n",
        "    print(f\"out.shape = {out.shape} , self.lora(input_tensor).shape = {self.lora(input_tensor).shape}\")\n",
        "    out = (out + self.lora(input_tensor))/2.0\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Az9-G-otqyuQ"
      },
      "outputs": [],
      "source": [
        "def create_factorized_compression_for_linear(source_linear, rank=None, rank_factor=0.3,  dtype=torch.float32):\n",
        "    with torch.no_grad():\n",
        "      if rank is None:\n",
        "        rank = max(1, int(min(source_linear.weight.shape)*rank_factor))\n",
        "      if hasattr(source_linear, 'bias'):\n",
        "        bias = source_linear.bias\n",
        "      else:\n",
        "        bias = None\n",
        "      source_linear = source_linear.weight.data\n",
        "      device=source_linear.device\n",
        "      assert rank < min(source_linear.shape)\n",
        "      source_linear = source_linear.float()\n",
        "      U, S, Vh = torch.linalg.svd(source_linear)\n",
        "      U = U[:, :rank]\n",
        "      S = S[:rank]\n",
        "      U = U @ torch.diag(S)\n",
        "      Vh = Vh[:rank, :]\n",
        "      U_flatten = U.flatten()\n",
        "      Vh_flatten = Vh.flatten()\n",
        "      max_quant_size = 2^23\n",
        "      #print (\"ranked\")\n",
        "      if len(U_flatten) + len(Vh_flatten) >= max_quant_size:\n",
        "        dist2 = U_flatten[:min(len(U_flatten), max_quant_size)]\n",
        "        dist3 = Vh_flatten[:min(len(Vh_flatten), max_quant_size)]\n",
        "        hi_val = max(torch.quantile(dist3, 1), torch.quantile(dist2, 1))\n",
        "      else:\n",
        "        dist = torch.cat([U_flatten, Vh_flatten])\n",
        "        hi_val = torch.quantile(dist, 1)\n",
        "      low_val = -hi_val\n",
        "      #print (\"quantile\")\n",
        "      U = U.clamp(low_val, hi_val)\n",
        "      Vh = Vh.clamp(low_val, hi_val)\n",
        "      #print (\"clammped\")\n",
        "      lora_down = nn.Linear(Vh.shape[1], Vh.shape[0], dtype=dtype, bias=False, device=source_linear.device)\n",
        "      lora_up = nn.Linear(U.shape[1], U.shape[0], dtype=dtype, bias=bias is not None, device=source_linear.device)\n",
        "      #print (\"Set up linear\")\n",
        "      lora_up.weight.data = U.to(device=device, dtype=dtype)\n",
        "      lora_down.weight.data = Vh.to(device=device, dtype=dtype)\n",
        "      if bias is not None:\n",
        "        lora_up.bias = nn.Parameter(bias.clone())\n",
        "      return nn.Sequential(lora_down, lora_up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v01GGPjNqw4B"
      },
      "outputs": [],
      "source": [
        "import cupy\n",
        "\n",
        "def lord_decompose(layer, proxy_data, rank):\n",
        "    \"\"\"\n",
        "    Be aware when performing LoRD/AFM on Square Matrices:\n",
        "\n",
        "        In square matrices, input and output dimensions are coupled so compression is more challenging.\n",
        "        The optimal low rank approximation may differ significantly from the original matrix due to the coupling.\n",
        "        Square matrices have less intrinsic redundancy between inputs and outputs to exploit.\n",
        "        Decomposing square matrices risks distorting dimensions that interact in complex ways.\n",
        "        The approximation error of AFM tends to be lowest for tall matrices and highest for square ones.\n",
        "        For square matrices, it can help to decompose blocks of interactions rather than the whole matrix.\n",
        "\n",
        "    Paper reference:\n",
        "        Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression\n",
        "        ( http://arxiv.org/abs/2309.14021 )\n",
        "\n",
        "    Credit: AI chatbot\n",
        "    \"\"\"\n",
        "\n",
        "    y = layer(proxy_data) # Forward proxy data\n",
        "\n",
        "    cov_y = torch.cov(y.T) # Compute output covariance\n",
        "    cov_y = cov_y.float()\n",
        "\n",
        "    eigenvalues, eigenvectors = torch.linalg.eig(cov_y)\n",
        "\n",
        "    # Allocate CuPy array\n",
        "    eigenvalues_cupy = cupy.empty(eigenvalues.shape, dtype=cupy.complex64)\n",
        "\n",
        "    # Copy with CUDA stream\n",
        "    eigenvalues_cupy = cupy.from_dlpack(torch.utils.dlpack.to_dlpack(eigenvalues))\n",
        "\n",
        "    # Take top rank eigenvectors in descending order\n",
        "    # [-rank:] selects the last rank indices, which correspond to the largest rank values.\n",
        "    # [::-1] then reverses the array to give the indices that would sort eigenvalues_cupy in descending order.\n",
        "    top_idx = cupy.argsort(cupy.abs(eigenvalues_cupy))[-rank:][::-1]\n",
        "\n",
        "    # PyTorch (eigenvectors) and CuPy (top_idx) arrays can't be indexed against each other like this due to incompatible types.\n",
        "    top_idx_pt = torch.from_numpy(top_idx.get())\n",
        "    U = eigenvectors[:, top_idx_pt]\n",
        "\n",
        "    # Convert layer weight to complex before decomposition\n",
        "    layer.weight = nn.Parameter(layer.weight.to(torch.complex64))\n",
        "\n",
        "    # Decompose\n",
        "    w1 = U.T @ layer.weight\n",
        "    w2 = U\n",
        "    print(f\"w1.shape = {w1.shape}\")\n",
        "    print(f\"w2.shape = {w2.shape}\")\n",
        "\n",
        "    # Create LoRD layers\n",
        "    lord_up = nn.Linear(w1.shape[0], w1.shape[1])\n",
        "    lord_down = nn.Linear(w2.shape[0], w2.shape[1])\n",
        "\n",
        "    lord_up.weight.data = torch.real(torch.abs(w1))\n",
        "    lord_down.weight.data = torch.real(torch.abs(w2))\n",
        "\n",
        "    return nn.Sequential(lord_down, lord_up)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "b13bb7bbc46c465ba44aae475b645679",
            "0c5b0a0d6de1470989f364629debbbe7",
            "e807febf3a994ba69f5f8da4fb1bae8e",
            "45b12a154ea94c1e8a72aa7a6d54ee60",
            "dd7f90dc847449618d31d92580a9b93c",
            "35427488be7246ad98823058cbf7daa6",
            "242a934518fb4d7581cfc80fa77f6dd4",
            "a076e85fc3ff451ea5c6c42b468f686a",
            "ad5af8a9581b4954ae94e61a9c189da1",
            "38fbee33279b438895dff3e62b2cc688",
            "73a5821f16134035aa51344aead6a961",
            "a8d461015f6a4abfb7525beb69d9961e",
            "cd80b76b6912437292a0f7841e9684bb",
            "ecd23ba92eae4233b5db7e26d9ef49f6",
            "b703c7197c084ff9b5c3a92b34911c8b",
            "1402860d89fb4b5fb287dbc0d73a659b",
            "0de0035575844b96a6351fe27815f422",
            "2e02dfa5a5d54c1eb8cb8e800dcb2af3",
            "4ef85d0fecaa48bdac6d92c0d61333bd",
            "8f5f97b91fa24e2c9d8cb2b65c596ea5",
            "50fddc5b8e404433aae338d026c6efa2",
            "dcdc436d9aee40b2b9a356e65c468e27",
            "2d6121bfcfa84b309a85149a81cf5433",
            "c50baf02f7414827bc856883349fea3c",
            "644b864d90a945a386b96abf4877ed8a",
            "4cc2fa33cada4bcbb32b54a242e2c63b",
            "2e8d3ef8559a416cb4973568a0e044c3",
            "22ea18c1fac34f55b81c2ee578017f95",
            "ac180de6a7264642b96c81394ec6c523",
            "caba9e605a424957b81de9d996fb00b4",
            "e1ae875105114a5d85990c5e16a26ed2",
            "3c0026462e7e4273b2d8650cd93070c2",
            "60db148f112e49008c39bd35a5043d90",
            "edfd1bbf53e149c7a20fb2b6cb417a29",
            "0ab94b7164b74097883926569884d9af",
            "d564fd6b7f094dd58eac0cf6ec80f690",
            "82a7aec467bd406f8eb32e845928d8ee",
            "cd12e64d443f4a9e852111805f80ce83",
            "da781c4d614a4f25a053477ab1de2f0e",
            "1a29afb008ae4fe8b3fdb8a4ad0e0745",
            "104c566442264072983f42ccb688b7e0",
            "747f57f88cd949feb665d302565fa190",
            "49bf3e8a1f524bc298079a77731e0059",
            "da86ebc2880b427f9b69f0714beba6ce",
            "65fac1ef77c54702863c53b9b2f504c7",
            "105d6202a9584518b00f890b071d3e05",
            "4c515f4919ce46a1828c706d4070a3b9",
            "944f2d21f788462fbe036fce51b0916d",
            "630a9d1531e3482ca2d5d4f671c410bb",
            "5c9ec8688a2c4bf497e1f60c22329ee1",
            "fd811202b80547b78a654ec53e7e11a0",
            "99cc1dcde0c649ad905cc26649218fe1",
            "8fccecfa414f47ca9c19e5933d0da8d2",
            "267844baad0e4f8a9e10ca6df1e6df79",
            "bbec1249a3ed43379d9de93219162a56"
          ]
        },
        "id": "KRL54Os9tDN_",
        "outputId": "07d0da45-56ab-4f63-bd88-75bf4493fa29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b13bb7bbc46c465ba44aae475b645679"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/911M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8d461015f6a4abfb7525beb69d9961e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/396 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d6121bfcfa84b309a85149a81cf5433"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "edfd1bbf53e149c7a20fb2b6cb417a29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65fac1ef77c54702863c53b9b2f504c7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from typing import Optional, Tuple, Union\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers.models.gpt_neox.configuration_gpt_neox import *\n",
        "\n",
        "\n",
        "model_name = \"EleutherAI/pythia-410m\" #\"EleutherAI/pythia-70m\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).cuda()\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "#model.embed_out  = create_factorized_compression_for_linear(model.embed_out, rank_factor=0.9).cuda().to(torch.bfloat16)\n",
        "#model.gpt_neox.embed_in = create_factorized_compression_for_linear(model.gpt_neox.embed_in, rank_factor=0.2).cuda().to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qnwVwUyeq-SV"
      },
      "outputs": [],
      "source": [
        "# Generate proxy dataset\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "\n",
        "proxy_texts = [\n",
        "  \"The cat sat on the mat.\",\n",
        "  \"The quick brown fox jumps over the lazy dog.\",\n",
        "  \"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\",\n",
        "  \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\",\n",
        "  \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\",\n",
        "  \"In a hole in the ground there lived a hobbit.\",\n",
        "  \"Happy families are all alike; every unhappy family is unhappy in its own way.\",\n",
        "  \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of light, it was the season of darkness, it was the spring of hope, it was the winter of despair.\",\n",
        "  \"I was born in the year 1632, in the city of York, of a good family, though not of that country, my father being a foreigner of Bremen, who settled first at Hull.\",\n",
        "  \"Many years later, as he faced the firing squad, Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice.\",\n",
        "  \"The sun shone, having no alternative, on the nothing new.\",\n",
        "  \"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\",\n",
        "  \"When he was nearly thirteen, my brother Jem got his arm badly broken at the elbow.\",\n",
        "  \"There was a boy called Eustace Clarence Scrubb, and he almost deserved it.\",\n",
        "  \"The primroses were over.\",\n",
        "  \"Many years later, as he faced the firing squad, Colonel Aureliano Buendía was to remember that distant afternoon when his father took him to discover ice.\",\n",
        "  \"Riverrun, past Eve and Adam's, from swerve of shore to bend of bay, brings us by a commodius vicus of recirculation back to Howth Castle and Environs.\",\n",
        "  \"124 was spiteful.\",\n",
        "  \"The cold passed reluctantly from the earth, and the retiring fogs revealed an army stretched out on the hills, resting.\",\n",
        "  \"We shall fight on the beaches, we shall fight on the landing grounds, we shall fight in the fields and in the streets, we shall fight in the hills; we shall never surrender.\",\n",
        "  \"I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.\",\n",
        "  \"Ask not what your country can do for you – ask what you can do for your country.\",\n",
        "  \"The only thing we have to fear is fear itself.\",\n",
        "  \"I do not agree with what you have to say, but I'll defend to the death your right to say it.\",\n",
        "  \"The future belongs to those who believe in the beauty of their dreams.\",\n",
        "  \"It does not matter how slowly you go as long as you do not stop.\"\n",
        "  \"Be who you are and say what you feel, because those who mind don't matter and those who matter don't mind.\",\n",
        "  \"We hold these truths to be self-evident, that all men are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Life, Liberty and the pursuit of Happiness.\",\n",
        "  # Add more samples from diverse books, speeches, genres etc.\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8QQZuy6HqzQs",
        "outputId": "03b70cb3-5a14-48ad-ae3a-9021eb3399b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "now processing layer [0]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1143: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:276.)\n",
            "  return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [2]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [4]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [6]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [8]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [10]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [12]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [14]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [16]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [18]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [20]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [22]\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "w1.shape = torch.Size([512, 1024])\n",
            "w2.shape = torch.Size([4096, 512])\n",
            "w1.shape = torch.Size([512, 4096])\n",
            "w2.shape = torch.Size([1024, 512])\n",
            "now processing layer [0]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5a78b499290>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       layer.attention.dense = LoraLinear(layer.attention.dense.in_features, layer.attention.dense.out_features, layer.attention.dense.bias is not None,  layer.attention.dense, \\\n\u001b[0;32m---> 29\u001b[0;31m                                         lord_decompose(layer.attention.dense, proxy_data_h_bf16, rank)).cuda().to(torch.bfloat16)\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0;31m#layer.attention.query_key_value = LoraLinear(layer.attention.query_key_value.in_features, layer.attention.query_key_value.out_features, layer.attention.query_key_value.bias is not None, layer.attention.query_key_value, \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9981f9a7491c>\u001b[0m in \u001b[0;36mlord_decompose\u001b[0;34m(layer, proxy_data, rank)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Forward proxy data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcov_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Compute output covariance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-0ac7b7d608e0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"out.shape = {out.shape} , self.lora(input_tensor).shape = {self.lora(input_tensor).shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (27x1024 and 512x1024)"
          ]
        }
      ],
      "source": [
        "USE_LORD = 1\n",
        "\n",
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "  if USE_LORD:\n",
        "    rank_factor=0.5\n",
        "\n",
        "    #for layer in model.gpt_neox.layers:\n",
        "    for i in range(0, len(model.gpt_neox.layers), 2):\n",
        "      layer = model.gpt_neox.layers[i]\n",
        "\n",
        "      print(f\"now processing layer [{i}]\")\n",
        "\n",
        "      input_dim = layer.attention.dense.in_features\n",
        "      rank = int(input_dim * rank_factor)\n",
        "\n",
        "\n",
        "      max_length_dense_h_to_4h = layer.mlp.dense_h_to_4h.in_features # Max sequence length\n",
        "      proxy_data_h = tokenizer(proxy_texts, padding=\"max_length\", truncation=True, max_length=max_length_dense_h_to_4h, return_tensors=\"pt\").to(\"cuda\")\n",
        "      proxy_data_h_bf16 = proxy_data_h['input_ids'].to(torch.bfloat16)\n",
        "\n",
        "      max_length_dense_4h_to_h = layer.mlp.dense_4h_to_h.in_features # Max sequence length\n",
        "      proxy_data_4h = tokenizer(proxy_texts, padding=\"max_length\", truncation=True, max_length=max_length_dense_4h_to_h, return_tensors=\"pt\").to(\"cuda\")\n",
        "      proxy_data_4h_bf16 = proxy_data_4h['input_ids'].to(torch.bfloat16)\n",
        "\n",
        "\n",
        "      layer.attention.dense = LoraLinear(layer.attention.dense.in_features, layer.attention.dense.out_features, layer.attention.dense.bias is not None,  layer.attention.dense, \\\n",
        "                                        lord_decompose(layer.attention.dense, proxy_data_h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      #layer.attention.query_key_value = LoraLinear(layer.attention.query_key_value.in_features, layer.attention.query_key_value.out_features, layer.attention.query_key_value.bias is not None, layer.attention.query_key_value, \\\n",
        "      #                                   lord_decompose(layer.attention.query_key_value, proxy_data, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      layer.mlp.dense_h_to_4h = LoraLinear(layer.mlp.dense_h_to_4h.in_features, layer.mlp.dense_h_to_4h.out_features, layer.mlp.dense_h_to_4h.bias is not None, layer.mlp.dense_h_to_4h, \\\n",
        "                                        lord_decompose(layer.mlp.dense_h_to_4h, proxy_data_h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      layer.mlp.dense_4h_to_h = LoraLinear(layer.mlp.dense_4h_to_h.in_features, layer.mlp.dense_4h_to_h.out_features, layer.mlp.dense_4h_to_h.bias is not None, layer.mlp.dense_4h_to_h, \\\n",
        "                                        lord_decompose(layer.mlp.dense_4h_to_h, proxy_data_4h_bf16, rank)).cuda().to(torch.bfloat16)\n",
        "\n",
        "  else:\n",
        "\n",
        "    #for layer in model.gpt_neox.layers:\n",
        "    for i in range(0, len(model.gpt_neox.layers), 2):\n",
        "      layer = model.gpt_neox.layers[i]\n",
        "\n",
        "      print(f\"now processing layer [{i}]\")\n",
        "\n",
        "      layer.attention.dense = LoraLinear(layer.attention.dense.in_features, layer.attention.dense.out_features, layer.attention.dense.bias is not None,  layer.attention.dense, \\\n",
        "                                        create_factorized_compression_for_linear(layer.attention.dense, rank_factor=0.2)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      #layer.attention.query_key_value = LoraLinear(layer.attention.query_key_value.in_features, layer.attention.query_key_value.out_features, layer.attention.query_key_value.bias is not None, layer.attention.query_key_value, \\\n",
        "      #                                   create_factorized_compression_for_linear(layer.attention.query_key_value, rank_factor=0.5)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      layer.mlp.dense_h_to_4h = LoraLinear(layer.mlp.dense_h_to_4h.in_features, layer.mlp.dense_h_to_4h.out_features, layer.mlp.dense_h_to_4h.bias is not None, layer.mlp.dense_h_to_4h, \\\n",
        "                                        create_factorized_compression_for_linear(layer.mlp.dense_h_to_4h, rank_factor=0.5)).cuda().to(torch.bfloat16)\n",
        "\n",
        "      layer.mlp.dense_4h_to_h = LoraLinear(layer.mlp.dense_4h_to_h.in_features, layer.mlp.dense_4h_to_h.out_features, layer.mlp.dense_4h_to_h.bias is not None, layer.mlp.dense_4h_to_h, \\\n",
        "                                        create_factorized_compression_for_linear(layer.mlp.dense_4h_to_h, rank_factor=0.5)).cuda().to(torch.bfloat16)\n",
        "\n",
        "\n",
        "input_ids = tokenizer(txt, return_tensors=\"pt\").to('cuda')\n",
        "print (input_ids.input_ids.shape)\n",
        "with torch.no_grad():\n",
        "  print (tokenizer.batch_decode(model.generate(**input_ids,  no_repeat_ngram_size=2, repetition_penalty=1.1, min_length=input_ids.input_ids.shape[1]+256, max_new_tokens=512))[0])\n",
        "\n",
        "model2 = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).cuda()\n",
        "\n",
        "\n",
        "print ('compression', sum(x.shape[0]*x.shape[1] if len(x.shape) == 2 else x.shape[0] for x in model.parameters())/ sum(x.shape[0]*x.shape[1] if len(x.shape) == 2 else x.shape[0] for x in model2.parameters()))\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pG8Dbwhqten"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFUaGmHxkA5h"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b13bb7bbc46c465ba44aae475b645679": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c5b0a0d6de1470989f364629debbbe7",
              "IPY_MODEL_e807febf3a994ba69f5f8da4fb1bae8e",
              "IPY_MODEL_45b12a154ea94c1e8a72aa7a6d54ee60"
            ],
            "layout": "IPY_MODEL_dd7f90dc847449618d31d92580a9b93c"
          }
        },
        "0c5b0a0d6de1470989f364629debbbe7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35427488be7246ad98823058cbf7daa6",
            "placeholder": "​",
            "style": "IPY_MODEL_242a934518fb4d7581cfc80fa77f6dd4",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "e807febf3a994ba69f5f8da4fb1bae8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a076e85fc3ff451ea5c6c42b468f686a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad5af8a9581b4954ae94e61a9c189da1",
            "value": 570
          }
        },
        "45b12a154ea94c1e8a72aa7a6d54ee60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38fbee33279b438895dff3e62b2cc688",
            "placeholder": "​",
            "style": "IPY_MODEL_73a5821f16134035aa51344aead6a961",
            "value": " 570/570 [00:00&lt;00:00, 44.8kB/s]"
          }
        },
        "dd7f90dc847449618d31d92580a9b93c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35427488be7246ad98823058cbf7daa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "242a934518fb4d7581cfc80fa77f6dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a076e85fc3ff451ea5c6c42b468f686a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad5af8a9581b4954ae94e61a9c189da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38fbee33279b438895dff3e62b2cc688": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73a5821f16134035aa51344aead6a961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8d461015f6a4abfb7525beb69d9961e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd80b76b6912437292a0f7841e9684bb",
              "IPY_MODEL_ecd23ba92eae4233b5db7e26d9ef49f6",
              "IPY_MODEL_b703c7197c084ff9b5c3a92b34911c8b"
            ],
            "layout": "IPY_MODEL_1402860d89fb4b5fb287dbc0d73a659b"
          }
        },
        "cd80b76b6912437292a0f7841e9684bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0de0035575844b96a6351fe27815f422",
            "placeholder": "​",
            "style": "IPY_MODEL_2e02dfa5a5d54c1eb8cb8e800dcb2af3",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "ecd23ba92eae4233b5db7e26d9ef49f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ef85d0fecaa48bdac6d92c0d61333bd",
            "max": 911373632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f5f97b91fa24e2c9d8cb2b65c596ea5",
            "value": 911373632
          }
        },
        "b703c7197c084ff9b5c3a92b34911c8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50fddc5b8e404433aae338d026c6efa2",
            "placeholder": "​",
            "style": "IPY_MODEL_dcdc436d9aee40b2b9a356e65c468e27",
            "value": " 911M/911M [00:03&lt;00:00, 280MB/s]"
          }
        },
        "1402860d89fb4b5fb287dbc0d73a659b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0de0035575844b96a6351fe27815f422": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e02dfa5a5d54c1eb8cb8e800dcb2af3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ef85d0fecaa48bdac6d92c0d61333bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5f97b91fa24e2c9d8cb2b65c596ea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50fddc5b8e404433aae338d026c6efa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcdc436d9aee40b2b9a356e65c468e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d6121bfcfa84b309a85149a81cf5433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c50baf02f7414827bc856883349fea3c",
              "IPY_MODEL_644b864d90a945a386b96abf4877ed8a",
              "IPY_MODEL_4cc2fa33cada4bcbb32b54a242e2c63b"
            ],
            "layout": "IPY_MODEL_2e8d3ef8559a416cb4973568a0e044c3"
          }
        },
        "c50baf02f7414827bc856883349fea3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22ea18c1fac34f55b81c2ee578017f95",
            "placeholder": "​",
            "style": "IPY_MODEL_ac180de6a7264642b96c81394ec6c523",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "644b864d90a945a386b96abf4877ed8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_caba9e605a424957b81de9d996fb00b4",
            "max": 396,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1ae875105114a5d85990c5e16a26ed2",
            "value": 396
          }
        },
        "4cc2fa33cada4bcbb32b54a242e2c63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c0026462e7e4273b2d8650cd93070c2",
            "placeholder": "​",
            "style": "IPY_MODEL_60db148f112e49008c39bd35a5043d90",
            "value": " 396/396 [00:00&lt;00:00, 6.16kB/s]"
          }
        },
        "2e8d3ef8559a416cb4973568a0e044c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ea18c1fac34f55b81c2ee578017f95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac180de6a7264642b96c81394ec6c523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "caba9e605a424957b81de9d996fb00b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1ae875105114a5d85990c5e16a26ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3c0026462e7e4273b2d8650cd93070c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60db148f112e49008c39bd35a5043d90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "edfd1bbf53e149c7a20fb2b6cb417a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ab94b7164b74097883926569884d9af",
              "IPY_MODEL_d564fd6b7f094dd58eac0cf6ec80f690",
              "IPY_MODEL_82a7aec467bd406f8eb32e845928d8ee"
            ],
            "layout": "IPY_MODEL_cd12e64d443f4a9e852111805f80ce83"
          }
        },
        "0ab94b7164b74097883926569884d9af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da781c4d614a4f25a053477ab1de2f0e",
            "placeholder": "​",
            "style": "IPY_MODEL_1a29afb008ae4fe8b3fdb8a4ad0e0745",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "d564fd6b7f094dd58eac0cf6ec80f690": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_104c566442264072983f42ccb688b7e0",
            "max": 2113710,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_747f57f88cd949feb665d302565fa190",
            "value": 2113710
          }
        },
        "82a7aec467bd406f8eb32e845928d8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49bf3e8a1f524bc298079a77731e0059",
            "placeholder": "​",
            "style": "IPY_MODEL_da86ebc2880b427f9b69f0714beba6ce",
            "value": " 2.11M/2.11M [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "cd12e64d443f4a9e852111805f80ce83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da781c4d614a4f25a053477ab1de2f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a29afb008ae4fe8b3fdb8a4ad0e0745": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "104c566442264072983f42ccb688b7e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "747f57f88cd949feb665d302565fa190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49bf3e8a1f524bc298079a77731e0059": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da86ebc2880b427f9b69f0714beba6ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65fac1ef77c54702863c53b9b2f504c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_105d6202a9584518b00f890b071d3e05",
              "IPY_MODEL_4c515f4919ce46a1828c706d4070a3b9",
              "IPY_MODEL_944f2d21f788462fbe036fce51b0916d"
            ],
            "layout": "IPY_MODEL_630a9d1531e3482ca2d5d4f671c410bb"
          }
        },
        "105d6202a9584518b00f890b071d3e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9ec8688a2c4bf497e1f60c22329ee1",
            "placeholder": "​",
            "style": "IPY_MODEL_fd811202b80547b78a654ec53e7e11a0",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "4c515f4919ce46a1828c706d4070a3b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99cc1dcde0c649ad905cc26649218fe1",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fccecfa414f47ca9c19e5933d0da8d2",
            "value": 99
          }
        },
        "944f2d21f788462fbe036fce51b0916d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_267844baad0e4f8a9e10ca6df1e6df79",
            "placeholder": "​",
            "style": "IPY_MODEL_bbec1249a3ed43379d9de93219162a56",
            "value": " 99.0/99.0 [00:00&lt;00:00, 2.83kB/s]"
          }
        },
        "630a9d1531e3482ca2d5d4f671c410bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9ec8688a2c4bf497e1f60c22329ee1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd811202b80547b78a654ec53e7e11a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99cc1dcde0c649ad905cc26649218fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fccecfa414f47ca9c19e5933d0da8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "267844baad0e4f8a9e10ca6df1e6df79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbec1249a3ed43379d9de93219162a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
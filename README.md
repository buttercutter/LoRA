# LoRA
LoRA: Low-Rank Adaptation of Large Language Models 

Note:
- Added [LORD: Low Rank Decomposition Of Monolingual Code LLMs For One-Shot Compression](http://arxiv.org/abs/2309.14021) in `lora_decompose()`

TODO:
- MoLoRA (Mixture of Experts for LoRA)
- [VeRA: Vector-based Random Matrix Adaptation](https://github.com/buttercutter/lora/tree/VeRA) coding implementation works well without using their suggested initialization strategies, so may need some more checking

Credit: AI chatbot, [@Ayush Kaushal](https://github.com/Ayushk4), [@ontocord](https://github.com/ontocord/) , [@cloneofsimo](https://github.com/cloneofsimo/)
